# üìä An√°lisis de Componentes Principales (PCA) en Python

Implementaci√≥n completa, manual y educativa del **An√°lisis de Componentes Principales (Principal Component Analysis, PCA)** usando Python.

Este proyecto permite:

‚úî Preprocesamiento autom√°tico de datos  
‚úî Codificaci√≥n de variables categ√≥ricas  
‚úî Estandarizaci√≥n  
‚úî C√°lculo manual de autovalores y autovectores  
‚úî Visualizaci√≥n avanzada  
‚úî Exportaci√≥n de resultados  

Especialmente √∫til para **agricultura, ecolog√≠a, biotecnolog√≠a y an√°lisis experimental**.

---

# üß† ¬øQu√© es el PCA?

El **An√°lisis de Componentes Principales (PCA)** es una t√©cnica estad√≠stica de **reducci√≥n de dimensionalidad** que transforma un conjunto de variables correlacionadas en un nuevo conjunto de variables no correlacionadas llamadas:

### üëâ Componentes principales (PCs)

Cada componente:

- es combinaci√≥n lineal de las variables originales
- explica la m√°xima varianza posible
- es ortogonal a los dem√°s

---

# üéØ Objetivos del PCA

- Reducir n√∫mero de variables
- Detectar patrones
- Eliminar colinealidad
- Visualizar datos multivariados
- Identificar variables m√°s influyentes

---

# üìê Fundamento matem√°tico

Sea una matriz de datos estandarizados:

\[
X_{n \times p}
\]

## 1Ô∏è‚É£ Matriz de correlaci√≥n/covarianza
\[
R = \frac{1}{n-1} X^T X
\]

## 2Ô∏è‚É£ Descomposici√≥n espectral
\[
R v = \lambda v
\]

donde:

- \( \lambda \) = autovalores (varianza explicada)
- \( v \) = autovectores (direcciones principales)

## 3Ô∏è‚É£ Componentes principales
\[
Z = X V
\]

---

# ‚ö†Ô∏è Supuestos y consideraciones

Para aplicar PCA correctamente:

‚úÖ Variables num√©ricas o codificadas  
‚úÖ Relaciones lineales  
‚úÖ Escalas comparables (estandarizar)  
‚úÖ Tama√±o de muestra suficiente  
‚úÖ Correlaci√≥n entre variables  

### Recomendaciones
- usar matriz de correlaci√≥n si escalas distintas
- revisar outliers
- interpretar solo componentes con varianza relevante

---

# üìä ¬øC√≥mo interpretar los resultados?

## Scree Plot
- muestra varianza por componente
- buscar ‚Äúcodo‚Äù
- regla de Kaiser: autovalor > 1

## Varianza acumulada
- elegir n√∫mero de PCs para explicar 80‚Äì90%

## Biplot
- puntos = observaciones
- flechas = variables
- √°ngulo peque√±o ‚Üí alta correlaci√≥n
- direcciones opuestas ‚Üí correlaci√≥n negativa

## Cargas factoriales (loadings)
- |valor| alto ‚Üí mayor contribuci√≥n
- indica qu√© variables forman cada componente

## Scatter PC1 vs PC2
- agrupa muestras similares
- detecta clusters o outliers

---

# üß∞ Librer√≠as utilizadas

| Librer√≠a | Uso |
|---------|------|
| pandas | manejo de datos |
| numpy | √°lgebra lineal |
| matplotlib | gr√°ficos |
| seaborn | heatmaps |
| scikit-learn | estandarizaci√≥n y codificaci√≥n |
| openpyxl | exportar Excel |

---


